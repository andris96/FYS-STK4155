{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes based on lectures and youtube video: https://www.youtube.com/watch?v=CqOfi41LfDw&ab_channel=StatQuestwithJoshStarmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal approximation theorem\n",
    "The great efficacy of neural networks can largely be contributed to the universal approximation theorem, which can simply be written as:\n",
    "\n",
    "$$\n",
    "\\vert F(\\boldsymbol{x})-f(\\boldsymbol{x};\\boldsymbol{\\Theta})\\vert < \\epsilon \\hspace{0.1cm} \\forall \\boldsymbol{x}\\in[0,1]^d.\n",
    "$$\n",
    "\n",
    "Where $F(\\boldsymbol{x})$ is a continuous function and deterministic function defined on the unit cube in $d$-dimensions, $F : [0,1]^d \\rightarrow \\mathbb{R} $. We aim to approximate $F$ for any given small positive error tolerance $\\epsilon > 0$. $f(\\boldsymbol{x}; \\boldsymbol{\\Theta})$ is a one-layer hidden neural network with parameters $\\boldsymbol{\\Theta} = (\\boldsymbol{W},\\boldsymbol{b})$ where the weights $\\boldsymbol{W}\\in\\mathbb{R}^{m\\times n}$ and the biases $\\boldsymbol{b}\\in \\mathbb{R}^{n}$. \n",
    "\n",
    "The neural network $f$ takes the input and computes for each neuron the activation value $\\boldsymbol{z_i}$ as: \n",
    "$$\\boldsymbol{z_i} = \\boldsymbol{w_i}\\cdot \\boldsymbol{x} + b_i$$\n",
    "Where $\\boldsymbol{w_i}$ and $b_i$ are the weight vector and bias for each neuron $i$. This value is then for each neuron passed through a continuous sigmoidal function $\\sigma(\\boldsymbol{z})$. Otherwise known as an activation function. This function has the property: \n",
    "$$\n",
    "\\sigma(\\boldsymbol{z}) = \\left\\{\\begin{array}{cc} 1 & \\boldsymbol{z}\\rightarrow \\infty\\\\ 0 & \\boldsymbol{z} \\rightarrow -\\infty \\end{array}\\right.\n",
    "$$\n",
    "Such a function could be for example the standard logistic function: \n",
    "$$ \\sigma(\\boldsymbol{z}) = \\frac{1}{1+\\mathrm e^{-\\boldsymbol{z}}} $$\n",
    "The function introduces non-linearity, which enables the possibility of approximating functions of high complexity.\n",
    "\n",
    "By summing the outputs of all the neurons, weighted directly by the network's parameters, we approximate $F(\\boldsymbol{x})$. Specifically, if we denote each neuron's activated output as $\\sigma(z_i)$, where $z_i = \\boldsymbol{w_i} \\cdot \\boldsymbol{x} + b_i$, then the final output of the neural network can be expressed as:\n",
    "$$\n",
    "f(\\boldsymbol{x}, \\boldsymbol{\\Theta}) = \\sum_{i=1}^{m} \\sigma(z_i)\n",
    "$$\n",
    "Each neuron's contribution is implicitly scaled by the weights $\\boldsymbol{w_i}$ and biases $b_i$ in the network. Through careful adjustment of these parameters, we can control each neuron's impact on the overall approximation, allowing $f(\\boldsymbol{x}; \\boldsymbol{\\Theta})$ to closely approximate $F(\\boldsymbol{x})$ within the given error tolerance $\\epsilon$.\n",
    "\n",
    "In single-layer networks, the Universal Approximation Theorem guarantees that with sufficient neurons, the right weights $\\boldsymbol{W}$, and biases $\\boldsymbol{b}$, we can approximate any continuous function. For multi-layer networks, which often enhance approximation power, backpropagation enables efficient tuning of parameters across layers through gradient descent methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation\n",
    "Back propagation is a process in which we attempt to improve a neural network by adjusting the internal parameters through calculating gradients of the cost/loss function wrt these parameters. We examine the initial predictions of the network, and then work backwards from the output, adjusting each layer of neurons using the chain rule of calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "First we do a forward pass, where we have an input that goes trough the network and predicts an output. Again we're interested in the activation values, but now we look at multiple layers, which is determined by the immeadiately preceding layer. The expression for each neuron $j$ in layer $l$ is then:\n",
    "\n",
    "$$\n",
    "z_j^l = \\sum_{i=1}^{M_{l-1}}w_{ij}^la_i^{l-1}+b_j^l,\n",
    "$$\n",
    "\n",
    "Here $a_i^{l-1}$ is the output from the previous layer $\\sigma(z_j^l)$. The total number of neurons is $M_{l-1}$. The final layer gives the predicted outcome $\\hat{y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error calculation\n",
    "We then use a cost/loss function such as MSE or Cross-Entropy Loss to evaluate how far off the target our prediction is. Our goal is to minimize the function.\n",
    "\n",
    "The MSE cost function is defined as:\n",
    "\n",
    "$$\n",
    "{\\cal C}(\\boldsymbol{\\Theta})  =  \\frac{1}{2}\\sum_{i=1}^n\\left(y_i - \\tilde{y}_i\\right)^2,\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient calculation\n",
    "To adjust the weights and biases, we compute the gradient of the error function wrt each bias and weight. Some parameters contribute more to the error than others, which means that they need greater adjustments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient adjustment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
